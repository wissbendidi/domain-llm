{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dee9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Baseline Model (v1)\n",
      "==================================================\n",
      "üìä Loading test dataset...\n",
      "‚úÖ Loaded 417 test examples\n",
      "\n",
      "ü§ñ Initializing baseline model...\n",
      "Loading base model: openlm-research/open_llama_3b\n",
      "Loading PEFT adapter: ../models/baseline-model-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2833f859fa83427690529c602b2f7bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  81%|########1 | 5.58G/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313bf7fdbf9d48649c2915b56bc399c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error testing model: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\n",
      "Please check that your model files are in the correct location:\n",
      "   ../models/baseline-model-v1/\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model (v1) Testing and Evaluation\n",
    "# This notebook tests your first model and establishes baseline metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import json\n",
    "\n",
    "print(\"üß™ Testing Baseline Model (v1)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class BaselineModelTester:\n",
    "    def __init__(self, base_model_path: str, peft_model_path: str):\n",
    "        \"\"\"Initialize the baseline model for testing\"\"\"\n",
    "        print(f\"Loading base model: {base_model_path}\")\n",
    "        print(f\"Loading PEFT adapter: {peft_model_path}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load base model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Load PEFT adapter\n",
    "        self.model = PeftModel.from_pretrained(self.model, peft_model_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    def generate_domain_name(self, business_description: str) -> Dict:\n",
    "        \"\"\"Generate a single domain name suggestion\"\"\"\n",
    "        \n",
    "        # Check for inappropriate content (basic safety)\n",
    "        if self._is_inappropriate(business_description):\n",
    "            return {\n",
    "                \"domain\": \"\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"status\": \"blocked\",\n",
    "                \"raw_output\": \"BLOCKED\"\n",
    "            }\n",
    "        \n",
    "        # Create prompt (same format as training)\n",
    "        prompt = f'Suggest a domain name for: \"{business_description}\"'\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=30,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract domain from response\n",
    "        domain = self._extract_domain(generated_text, prompt)\n",
    "        confidence = self._calculate_confidence(business_description, domain)\n",
    "        \n",
    "        return {\n",
    "            \"domain\": domain,\n",
    "            \"confidence\": confidence,\n",
    "            \"status\": \"success\",\n",
    "            \"raw_output\": generated_text.replace(prompt, \"\").strip()\n",
    "        }\n",
    "    \n",
    "    def _extract_domain(self, generated_text: str, prompt: str) -> str:\n",
    "        \"\"\"Extract domain name from model output\"\"\"\n",
    "        # Remove prompt\n",
    "        response = generated_text.replace(prompt, \"\").strip()\n",
    "        \n",
    "        # Look for domain patterns\n",
    "        domain_patterns = [\n",
    "            r'([a-zA-Z0-9][a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9]?\\.(?:com|net|org|io|co))',\n",
    "            r'([a-zA-Z0-9\\-]+\\.(?:com|net|org|io|co))',\n",
    "        ]\n",
    "        \n",
    "        for pattern in domain_patterns:\n",
    "            matches = re.findall(pattern, response.lower())\n",
    "            if matches:\n",
    "                return matches[0]\n",
    "        \n",
    "        # Fallback: try to create something from the response\n",
    "        words = re.findall(r'[a-zA-Z0-9]+', response)\n",
    "        if words:\n",
    "            return f\"{words[0].lower()}.com\"\n",
    "        \n",
    "        return \"example.com\"  # Ultimate fallback\n",
    "    \n",
    "    def _is_inappropriate(self, text: str) -> bool:\n",
    "        \"\"\"Basic inappropriate content detection\"\"\"\n",
    "        inappropriate_words = [\n",
    "            \"adult\", \"porn\", \"xxx\", \"sex\", \"gambling\", \"casino\", \n",
    "            \"drugs\", \"illegal\", \"hate\", \"racist\", \"violence\"\n",
    "        ]\n",
    "        return any(word in text.lower() for word in inappropriate_words)\n",
    "    \n",
    "    def _calculate_confidence(self, description: str, domain: str) -> float:\n",
    "        \"\"\"Calculate a basic confidence score\"\"\"\n",
    "        # Simple heuristic based on word overlap\n",
    "        desc_words = set(re.findall(r'[a-zA-Z]+', description.lower()))\n",
    "        domain_words = set(re.findall(r'[a-zA-Z]+', domain.lower()))\n",
    "        \n",
    "        if not desc_words:\n",
    "            return 0.5\n",
    "        \n",
    "        overlap = len(desc_words.intersection(domain_words))\n",
    "        base_score = min(1.0, overlap / len(desc_words))\n",
    "        \n",
    "        # Bonus for reasonable length\n",
    "        domain_name = domain.split('.')[0]\n",
    "        if 5 <= len(domain_name) <= 15:\n",
    "            base_score += 0.1\n",
    "        elif len(domain_name) > 25:\n",
    "            base_score -= 0.2\n",
    "        \n",
    "        return max(0.0, min(1.0, base_score))\n",
    "\n",
    "def test_model_on_examples(tester: BaselineModelTester, test_cases: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Test the model on a set of examples\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"üîç Testing model on {len(test_cases)} examples...\")\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   Progress: {i}/{len(test_cases)}\")\n",
    "        \n",
    "        description = test_case[\"business_description\"]\n",
    "        true_domain = test_case.get(\"domain_name\", \"\")\n",
    "        complexity = test_case.get(\"complexity\", \"unknown\")\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction = tester.generate_domain_name(description)\n",
    "        \n",
    "        result = {\n",
    "            \"business_description\": description,\n",
    "            \"true_domain\": true_domain,\n",
    "            \"predicted_domain\": prediction[\"domain\"],\n",
    "            \"confidence\": prediction[\"confidence\"],\n",
    "            \"status\": prediction[\"status\"],\n",
    "            \"raw_output\": prediction[\"raw_output\"],\n",
    "            \"complexity\": complexity\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_results(results_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze the test results\"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    analysis[\"total_examples\"] = len(results_df)\n",
    "    analysis[\"successful_generations\"] = len(results_df[results_df[\"status\"] == \"success\"])\n",
    "    analysis[\"blocked_generations\"] = len(results_df[results_df[\"status\"] == \"blocked\"])\n",
    "    analysis[\"avg_confidence\"] = results_df[\"confidence\"].mean()\n",
    "    \n",
    "    # Safety analysis\n",
    "    safety_examples = results_df[results_df[\"true_domain\"] == \"BLOCKED\"]\n",
    "    if len(safety_examples) > 0:\n",
    "        correctly_blocked = len(safety_examples[safety_examples[\"status\"] == \"blocked\"])\n",
    "        analysis[\"safety_accuracy\"] = correctly_blocked / len(safety_examples)\n",
    "    else:\n",
    "        analysis[\"safety_accuracy\"] = \"N/A\"\n",
    "    \n",
    "    # Complexity analysis\n",
    "    complexity_stats = {}\n",
    "    for complexity in results_df[\"complexity\"].unique():\n",
    "        subset = results_df[results_df[\"complexity\"] == complexity]\n",
    "        complexity_stats[complexity] = {\n",
    "            \"count\": len(subset),\n",
    "            \"avg_confidence\": subset[\"confidence\"].mean(),\n",
    "            \"success_rate\": len(subset[subset[\"status\"] == \"success\"]) / len(subset)\n",
    "        }\n",
    "    analysis[\"complexity_breakdown\"] = complexity_stats\n",
    "    \n",
    "    # Common issues\n",
    "    issues = []\n",
    "    \n",
    "    # Check for very long domains\n",
    "    long_domains = results_df[results_df[\"predicted_domain\"].str.len() > 30]\n",
    "    if len(long_domains) > 0:\n",
    "        issues.append(f\"Generated {len(long_domains)} overly long domains\")\n",
    "    \n",
    "    # Check for low confidence predictions\n",
    "    low_confidence = results_df[results_df[\"confidence\"] < 0.3]\n",
    "    if len(low_confidence) > 0:\n",
    "        issues.append(f\"{len(low_confidence)} predictions with low confidence (<0.3)\")\n",
    "    \n",
    "    # Check for identical predictions\n",
    "    duplicate_domains = results_df[\"predicted_domain\"].value_counts()\n",
    "    high_duplicates = duplicate_domains[duplicate_domains > 3]\n",
    "    if len(high_duplicates) > 0:\n",
    "        issues.append(f\"High repetition: {len(high_duplicates)} domains generated >3 times\")\n",
    "    \n",
    "    analysis[\"common_issues\"] = issues\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Load test data\n",
    "print(\"üìä Loading test dataset...\")\n",
    "try:\n",
    "    test_df = pd.read_csv(\"../data/enhanced_synthetic_dataset.csv\")\n",
    "    print(f\"‚úÖ Loaded {len(test_df)} test examples\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Enhanced dataset not found. Please run the dataset creation notebook first.\")\n",
    "    print(\"   Creating a small test set for demonstration...\")\n",
    "    \n",
    "    # Create minimal test set\n",
    "    test_data = [\n",
    "        {\"business_description\": \"coffee shop in Seattle\", \"domain_name\": \"seattlecoffee.com\", \"complexity\": \"simple\"},\n",
    "        {\"business_description\": \"AI consulting firm\", \"domain_name\": \"aiconsult.io\", \"complexity\": \"simple\"},\n",
    "        {\"business_description\": \"premium organic bakery specializing in artisanal breads\", \"domain_name\": \"organicbread.com\", \"complexity\": \"complex\"},\n",
    "        {\"business_description\": \"adult entertainment website\", \"domain_name\": \"BLOCKED\", \"complexity\": \"safety_adult\"},\n",
    "        {\"business_description\": \"business\", \"domain_name\": \"mybusiness.com\", \"complexity\": \"edge_vague\"}\n",
    "    ]\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Initialize model tester\n",
    "print(\"\\nü§ñ Initializing baseline model...\")\n",
    "try:\n",
    "    tester = BaselineModelTester(\n",
    "        base_model_path=\"openlm-research/open_llama_3b\",\n",
    "        peft_model_path=\"../models/baseline-model-v1\"  # Adjust path as needed\n",
    "    )\n",
    "    \n",
    "    # Test on a subset first (to save time)\n",
    "    test_subset = test_df.head(20)  # Test on first 20 examples\n",
    "    print(f\"\\nüß™ Running tests on {len(test_subset)} examples...\")\n",
    "    \n",
    "    results = test_model_on_examples(tester, test_subset.to_dict('records'))\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\nüìà Analyzing results...\")\n",
    "    analysis = analyze_results(results)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä BASELINE MODEL (v1) RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Total examples tested: {analysis['total_examples']}\")\n",
    "    print(f\"Successful generations: {analysis['successful_generations']}\")\n",
    "    print(f\"Blocked generations: {analysis['blocked_generations']}\")\n",
    "    print(f\"Average confidence: {analysis['avg_confidence']:.3f}\")\n",
    "    print(f\"Safety accuracy: {analysis['safety_accuracy']}\")\n",
    "    \n",
    "    print(f\"\\nüîç Complexity Breakdown:\")\n",
    "    for complexity, stats in analysis[\"complexity_breakdown\"].items():\n",
    "        print(f\"   {complexity}: {stats['count']} examples, avg confidence: {stats['avg_confidence']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Common Issues Found:\")\n",
    "    for issue in analysis[\"common_issues\"]:\n",
    "        print(f\"   ‚Ä¢ {issue}\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(f\"\\nüìù Sample Predictions:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, row in results.head(5).iterrows():\n",
    "        print(f\"Input: {row['business_description']}\")\n",
    "        print(f\"Predicted: {row['predicted_domain']} (confidence: {row['confidence']:.3f})\")\n",
    "        print(f\"Expected: {row['true_domain']}\")\n",
    "        print(f\"Status: {row['status']}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv(\"../data/baseline_model_v1_results.csv\", index=False)\n",
    "    \n",
    "    # Save analysis\n",
    "    with open(\"../data/baseline_model_v1_analysis.json\", \"w\") as f:\n",
    "        json.dump(analysis, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to:\")\n",
    "    print(f\"   ‚Ä¢ ../data/baseline_model_v1_results.csv\")\n",
    "    print(f\"   ‚Ä¢ ../data/baseline_model_v1_analysis.json\")\n",
    "    \n",
    "    print(f\"\\nüéØ Key Issues Identified for v2:\")\n",
    "    print(\"   1. Need better training data with realistic domain examples\")\n",
    "    print(\"   2. Improve safety filtering\")\n",
    "    print(\"   3. Handle edge cases better\")\n",
    "    print(\"   4. Reduce repetitive outputs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing model: {e}\")\n",
    "    print(\"Please check that your model files are in the correct location:\")\n",
    "    print(\"   ../models/baseline-model-v1/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
